# Handling JavaScript Single Page Applications (SPAs)

## The Problem

Traditional web crawlers parse HTML and extract `<a href>` links. But modern JavaScript frameworks (React, Vue, Angular, Docusaurus) render content dynamically via JavaScript. The HTML source contains empty containers like:

```html
<div id="__docusaurus"></div>
<script src="bundle.js"></script>
```

**Result:** Traditional crawlers see 0 links, only discover the homepage.

---

## Solution 1: Sitemap.xml Crawling ‚úÖ (RECOMMENDED)

**Best for:** Most SPAs, especially documentation sites (Docusaurus, Gatsby, Next.js)

### Why It Works
- Most modern frameworks auto-generate `sitemap.xml`
- Lists all pages in XML format
- Fast, lightweight, no JavaScript execution needed
- **808 pages discovered in 0.29 seconds** (learn.netdata.cloud)

### Implementation

```javascript
const { discoverFromSitemap } = require('./sitemap-crawler');

// Discover all pages from sitemap
const pages = await discoverFromSitemap('https://learn.netdata.cloud');
console.log(`Found ${pages.length} pages`); // 808 pages!
```

### Usage

```bash
# Test sitemap crawler
node test-sitemap-crawler.js
```

### Pros
‚úÖ Extremely fast (< 1 second)
‚úÖ Works with all SPAs that have sitemaps
‚úÖ No heavy dependencies
‚úÖ Respects robots.txt
‚úÖ Gets ALL pages, even deeply nested ones

### Cons
‚ö†Ô∏è Requires site to have sitemap.xml
‚ö†Ô∏è Won't discover pages not in sitemap

---

## Solution 2: Headless Browser Crawling üåê

**Best for:** Sites without sitemaps, complex JavaScript interactions

### How It Works
- Uses Puppeteer or Playwright
- Runs real browser (Chrome/Firefox)
- Executes JavaScript, waits for content to render
- Extracts links from fully rendered DOM

### Implementation (Puppeteer)

```javascript
const puppeteer = require('puppeteer');

async function crawlWithBrowser(url) {
  const browser = await puppeteer.launch();
  const page = await browser.newPage();

  await page.goto(url, { waitUntil: 'networkidle0' });

  // Extract links from rendered page
  const links = await page.$$eval('a[href]', anchors =>
    anchors.map(a => a.href)
  );

  await browser.close();
  return links;
}
```

### Pros
‚úÖ Works with ANY JavaScript site
‚úÖ Sees exactly what users see
‚úÖ Can interact with buttons, forms, etc.
‚úÖ Handles dynamic content loading

### Cons
‚ùå Very slow (3-10 seconds per page)
‚ùå Heavy resource usage (RAM, CPU)
‚ùå Requires Chrome/Chromium installation
‚ùå More complex to deploy

---

## Solution 3: Hybrid Approach ‚ö° (BEST OF BOTH WORLDS)

**Recommended strategy:** Try sitemap first, fallback to headless browser if needed

### Implementation

```javascript
async function smartCrawl(url) {
  console.log(`Crawling ${url}...`);

  // 1. Try traditional crawling first
  const pages = await crawlWebsite(url);

  // 2. If only 1 page found, try sitemap
  if (pages.length === 1) {
    console.log('‚ö†Ô∏è  SPA detected, trying sitemap...');
    const sitemapPages = await discoverFromSitemap(url);

    if (sitemapPages.length > 1) {
      console.log(`‚úÖ Found ${sitemapPages.length} pages via sitemap`);
      return sitemapPages;
    }
  }

  // 3. If sitemap fails, could try headless browser
  // (not implemented yet - would require Puppeteer)

  return pages;
}
```

---

## Comparison Table

| Approach | Speed | Coverage | Resources | Complexity |
|----------|-------|----------|-----------|------------|
| **Traditional Crawler** | ‚ö°‚ö°‚ö° Fast | ‚ùå SPAs: 0% | ‚úÖ Minimal | ‚úÖ Simple |
| **Sitemap Crawler** | ‚ö°‚ö°‚ö° Fast | ‚úÖ 100% (if sitemap exists) | ‚úÖ Minimal | ‚úÖ Simple |
| **Headless Browser** | üêå Slow | ‚úÖ 100% | ‚ùå Heavy | ‚ö†Ô∏è Complex |
| **Hybrid** | ‚ö°‚ö° Fast | ‚úÖ 95%+ | ‚úÖ Minimal | ‚ö†Ô∏è Medium |

---

## Detection Strategy

### How to detect if a site is a SPA:

```javascript
function isSPA(crawlResult) {
  // Only found 1 page with 0 links
  return crawlResult.pages.length === 1 &&
         crawlResult.pages[0].linksCount === 0;
}
```

### Common SPA frameworks:
- **Docusaurus** - Always has sitemap.xml ‚úÖ
- **Gatsby** - Usually has sitemap.xml ‚úÖ
- **Next.js** - Often has sitemap.xml ‚úÖ
- **Create React App** - No sitemap by default ‚ö†Ô∏è
- **Vue CLI** - No sitemap by default ‚ö†Ô∏è
- **Angular** - No sitemap by default ‚ö†Ô∏è

---

## Real-World Results

### www.netdata.cloud (Traditional site)
```
Method: Traditional crawler
Pages: 1,940
Time: ~120 seconds
Success: ‚úÖ Full coverage
```

### learn.netdata.cloud (Docusaurus SPA)
```
Method: Sitemap crawler
Pages: 808
Time: 0.29 seconds
Success: ‚úÖ Full coverage
```

---

## Recommendations

### For Link Checking (your use case)

1. **Try traditional crawler first** (covers 90% of sites)
2. **Detect SPA** (1 page, 0 links = SPA)
3. **Try sitemap.xml** (covers most modern SPAs)
4. **Report to user** if neither works

### When to use Headless Browser

Only use if:
- Site has no sitemap
- You MUST check the site
- You have resources (time, RAM)
- You need to test JavaScript interactions

### Best Practice

```javascript
// Automatic detection and fallback
async function intelligentCrawl(url) {
  const result = await crawlWebsite(url);

  if (result.pages.length === 1 && result.pages[0].linksCount === 0) {
    console.log('üîç SPA detected, checking for sitemap...');
    const sitemapPages = await discoverFromSitemap(url);

    if (sitemapPages.length > 1) {
      return { pages: sitemapPages, method: 'sitemap' };
    } else {
      console.log('‚ö†Ô∏è  No sitemap found. Consider using headless browser.');
      return { pages: result.pages, method: 'traditional', isSPA: true };
    }
  }

  return { pages: result.pages, method: 'traditional' };
}
```

---

## Summary

‚úÖ **Sitemap crawling** solves 95% of SPA crawling needs
‚úÖ **Fast** (< 1 second vs minutes with headless browser)
‚úÖ **Simple** (no complex dependencies)
‚úÖ **Reliable** (framework-generated, always up-to-date)

For `dead-links-must-die`, sitemap crawling is the perfect solution for SPAs like learn.netdata.cloud! üöÄ
