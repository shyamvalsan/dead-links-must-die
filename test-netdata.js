const { crawlWebsite } = require('./crawler');

async function testNetdataSite(url) {
  console.log(`\n${'='.repeat(80)}`);
  console.log(`ðŸ§ª Testing crawler with ${url}`);
  console.log('='.repeat(80));

  const startTime = Date.now();
  const pagesDiscovered = new Set();
  let lastProgressUpdate = Date.now();

  try {
    const result = await crawlWebsite(
      url,
      // Progress callback
      (progress) => {
        // Only log every 2 seconds to avoid spam
        if (Date.now() - lastProgressUpdate > 2000) {
          console.log(`ðŸ“Š Progress: ${progress.pagesCrawled}/${progress.pagesFound} pages crawled`);
          lastProgressUpdate = Date.now();
        }
      },
      // Page crawled callback
      (page) => {
        pagesDiscovered.add(page.url);
        if (pagesDiscovered.size <= 10 || pagesDiscovered.size % 10 === 0) {
          console.log(`âœ… [${pagesDiscovered.size}] ${page.url}`);
          console.log(`   Title: ${page.title.substring(0, 60)}${page.title.length > 60 ? '...' : ''}`);
          console.log(`   Links: ${page.linksCount}, Images: ${page.imagesCount}`);
        }
      }
    );

    const duration = ((Date.now() - startTime) / 1000).toFixed(2);

    console.log(`\n${'='.repeat(80)}`);
    console.log('ðŸ“Š CRAWL RESULTS');
    console.log('='.repeat(80));
    console.log(`âœ… Total pages discovered: ${result.pages.length}`);
    console.log(`âœ… Total URLs visited: ${result.crawledUrls.size}`);
    console.log(`â±ï¸  Time taken: ${duration}s`);

    // Calculate statistics
    const totalLinks = result.pages.reduce((sum, page) => sum + page.linksCount, 0);
    const totalImages = result.pages.reduce((sum, page) => sum + page.imagesCount, 0);
    const avgLinksPerPage = (totalLinks / result.pages.length).toFixed(1);

    console.log(`ðŸ“Š Total links found: ${totalLinks} (avg ${avgLinksPerPage} per page)`);
    console.log(`ðŸ–¼ï¸  Total images found: ${totalImages}`);

    // Show sample of discovered pages
    console.log(`\nðŸ“„ Sample of discovered pages (first 15):`);
    result.pages.slice(0, 15).forEach((page, i) => {
      const title = page.title.substring(0, 50) + (page.title.length > 50 ? '...' : '');
      console.log(`   ${i + 1}. ${page.url}`);
      console.log(`      "${title}"`);
    });

    if (result.pages.length > 15) {
      console.log(`   ... and ${result.pages.length - 15} more pages`);
    }

    return {
      success: result.pages.length > 1,
      pagesCount: result.pages.length,
      url: url
    };

  } catch (error) {
    console.error(`âŒ Test failed for ${url}:`, error.message);
    return {
      success: false,
      error: error.message,
      url: url
    };
  }
}

async function runTests() {
  console.log('\nðŸš€ NETDATA CRAWLER TEST SUITE');
  console.log('Testing the fixed crawler with real Netdata domains\n');

  const sites = [
    'https://netdata.cloud',
    'https://learn.netdata.cloud'
  ];

  const results = [];

  for (const site of sites) {
    const result = await testNetdataSite(site);
    results.push(result);

    // Small delay between tests
    await new Promise(resolve => setTimeout(resolve, 2000));
  }

  // Final summary
  console.log(`\n${'='.repeat(80)}`);
  console.log('ðŸ“‹ FINAL SUMMARY');
  console.log('='.repeat(80));

  results.forEach((result, i) => {
    console.log(`\n${i + 1}. ${result.url}`);
    if (result.success) {
      console.log(`   âœ… SUCCESS: Discovered ${result.pagesCount} pages`);
    } else {
      console.log(`   âŒ FAILED: ${result.error || 'Only found homepage'}`);
    }
  });

  const allSuccessful = results.every(r => r.success);
  const totalPages = results.reduce((sum, r) => sum + (r.pagesCount || 0), 0);

  console.log(`\n${'='.repeat(80)}`);
  if (allSuccessful) {
    console.log('âœ… ALL TESTS PASSED!');
    console.log(`âœ… The crawler successfully discovered multiple pages on all ${results.length} domains`);
    console.log(`âœ… Total pages crawled across all sites: ${totalPages}`);
    console.log('âœ… The bug is FIXED - crawler now properly discovers internal links!');
    process.exit(0);
  } else {
    console.log('âŒ SOME TESTS FAILED');
    console.log('âŒ The crawler may still have issues with certain sites');
    process.exit(1);
  }
}

runTests().catch(error => {
  console.error('Fatal error:', error);
  process.exit(1);
});
